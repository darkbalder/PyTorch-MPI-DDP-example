xhzhao@xhzhao-ub:~/tools/PyTorch-MPI-DDP-example$ ./run.sh 
[1/2] first broadcast start
[0/2] first broadcast start
[0/2] first broadcast done
[1/2] first broadcast done
[0/2] Epoch 0 Loss 0.956670 Global batch size 128 on 2 ranks
[1/2] Epoch 0 Loss 0.961719 Global batch size 128 on 2 ranks
[0/2] Epoch 1 Loss 0.405841 Global batch size 128 on 2 ranks
[1/2] Epoch 1 Loss 0.397616 Global batch size 128 on 2 ranks
[1/2] Epoch 2 Loss 0.318296 Global batch size 128 on 2 ranks
[0/2] Epoch 2 Loss 0.317754 Global batch size 128 on 2 ranks
[0/2] Epoch 3 Loss 0.272213 Global batch size 128 on 2 ranks
[1/2] Epoch 3 Loss 0.271408 Global batch size 128 on 2 ranks
[0/2] Epoch 4 Loss 0.242495 Global batch size 128 on 2 ranks
[1/2] Epoch 4 Loss 0.242542 Global batch size 128 on 2 ranks
[0/2] Epoch 5 Loss 0.219803 Global batch size 128 on 2 ranks
[1/2] Epoch 5 Loss 0.216617 Global batch size 128 on 2 ranks
[0/2] Epoch 6 Loss 0.206899 Global batch size 128 on 2 ranks
[1/2] Epoch 6 Loss 0.201472 Global batch size 128 on 2 ranks
[1/2] Epoch 7 Loss 0.197499 Global batch size 128 on 2 ranks
[0/2] Epoch 7 Loss 0.194447 Global batch size 128 on 2 ranks
[0/2] Epoch 8 Loss 0.185426 Global batch size 128 on 2 ranks
[1/2] Epoch 8 Loss 0.181102 Global batch size 128 on 2 ranks
[0/2] Epoch 9 Loss 0.177293 Global batch size 128 on 2 ranks
[1/2] Epoch 9 Loss 0.171482 Global batch size 128 on 2 ranks
